{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import essential libraries to load csv files and do preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Outcomes-a.txt' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e5144156bd54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstatic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mminimum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mlabel_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Outcomes-a.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RecordID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'In-hospital_death'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ICUType'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdrop_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RecordID'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__ (pandas/_libs/parsers.c:4209)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source (pandas/_libs/parsers.c:8873)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Outcomes-a.txt' does not exist"
     ]
    }
   ],
   "source": [
    "# load training data and do preprocessing\n",
    "mean = pd.DataFrame()\n",
    "maximum = pd.DataFrame()\n",
    "minimum = pd.DataFrame()\n",
    "for file in os.listdir('/Users/zhuangshui/Documents/3163/FIT3164-master/set-a/')[1:]:\n",
    "    df = pd.read_csv('/Users/zhuangshui/Documents/3163/FIT3164-master/set-a/'+file)\n",
    "    df = df.pivot(columns='Parameter',values='Value')\n",
    "    mean = mean.append(df.mean(),ignore_index=True)\n",
    "    maximum = maximum.append(df.max(),ignore_index=True)\n",
    "    minimum = minimum.append(df.min(),ignore_index=True)\n",
    "static = mean[['RecordID','Age','Height','Gender','ICUType']]\n",
    "mean = mean.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_mean')\n",
    "maximum = maximum.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_max')\n",
    "minimum = minimum.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_min')\n",
    "train = pd.concat([static,mean,maximum,minimum],axis=1)\n",
    "\n",
    "label_train = pd.read_csv('Outcomes-a.txt')[['RecordID','In-hospital_death']]\n",
    "train = pd.merge(pd.get_dummies(train,columns=['ICUType'],drop_first=True),label_train).drop('RecordID',axis=1)\n",
    "\n",
    "# load watching data and do preprocessing\n",
    "mean = pd.DataFrame()\n",
    "maximum = pd.DataFrame()\n",
    "minimum = pd.DataFrame()\n",
    "for file in os.listdir('/Users/zhuangshui/Documents/3163/FIT3164-master/set-b/')[1:]:\n",
    "    df = pd.read_csv('/Users/zhuangshui/Documents/3163/FIT3164-master/set-b/' + file)\n",
    "    df = df.pivot(columns='Parameter',values='Value')\n",
    "    mean = mean.append(df.mean(),ignore_index=True)\n",
    "    maximum = maximum.append(df.max(),ignore_index=True)\n",
    "    minimum = minimum.append(df.min(),ignore_index=True)\n",
    "static = mean[['RecordID','Age','Height','Gender','ICUType']]\n",
    "mean = mean.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_mean')\n",
    "maximum = maximum.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_max')\n",
    "minimum = minimum.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_min')\n",
    "watch = pd.concat([static,mean,maximum,minimum],axis=1,sort=False)\n",
    "\n",
    "label_watch = pd.read_csv('Outcomes-b.txt')[['RecordID','In-hospital_death']]\n",
    "watch = pd.merge(pd.get_dummies(watch,columns=['ICUType'],drop_first=True),label_watch).drop('RecordID',axis=1)\n",
    "\n",
    "# load testing data and do preprocessing\n",
    "mean = pd.DataFrame()\n",
    "maximum = pd.DataFrame()\n",
    "minimum = pd.DataFrame()\n",
    "for file in os.listdir('~/Users/zhuangshui/Documents/3163/FIT3164-master/set-c/')[1:]:\n",
    "    df = pd.read_csv('~/Users/zhuangshui/Documents/3163/FIT3164-master/set-c/' + file)\n",
    "    df = df.pivot(columns='Parameter',values='Value')\n",
    "    mean = mean.append(df.mean(),ignore_index=True)\n",
    "    maximum = maximum.append(df.max(),ignore_index=True)\n",
    "    minimum = minimum.append(df.min(),ignore_index=True)\n",
    "static = mean[['RecordID','Age','Height','Gender','ICUType']]\n",
    "mean = mean.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_mean')\n",
    "maximum = maximum.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_max')\n",
    "minimum = minimum.drop(['RecordID','Age','Height','Gender','ICUType'],axis=1).add_suffix('_min')\n",
    "test = pd.concat([static,mean,maximum,minimum],axis=1,sort=False)\n",
    "\n",
    "label_test = pd.read_csv('Outcomes-c.txt')[['RecordID','In-hospital_death']]\n",
    "test = pd.merge(pd.get_dummies(test,columns=['ICUType'],drop_first=True),label_test).drop('RecordID',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing is done by extracting the maximum, mean and minimum of the time series data. Combine the target variable based on RecordID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format three datasets containing same features, for missing values, impute with the population mean. For testing set, impute with the mean of training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = test.drop(list(set(test).difference(set(train))),axis=1)\n",
    "test = test.reindex(sorted(test.columns),axis=1)\n",
    "train = train.reindex(sorted(train.columns),axis=1)\n",
    "watch = watch.reindex(sorted(watch.columns),axis=1)\n",
    "train = train.replace(-1,np.NaN).fillna(train.mean())\n",
    "watch = watch.replace(-1,np.NaN).fillna(watch.mean())\n",
    "test = test.replace(-1,np.NaN).fillna(train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save three datasets into csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train.csv',index=False)\n",
    "watch.to_csv('watch.csv',index=False)\n",
    "test.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------- separate -----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load datasets, split datasets into features and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "watch = pd.read_csv('watch.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "train_X = train.drop('In-hospital_death',axis=1)\n",
    "train_y = train['In-hospital_death']\n",
    "watch_X = watch.drop('In-hospital_death',axis=1)\n",
    "watch_y = watch['In-hospital_death']\n",
    "test_X = test.drop('In-hospital_death',axis=1)\n",
    "test_y = test['In-hospital_death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load metric from sklearn and xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model on the full training dataset, evaluated on the watching dataset. manually select the hyperparameters to achieve an acceptable score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "model = xgb.XGBClassifier(max_depth=3,n_estimators=100,learning_rate=0.08,scale_pos_weight=3)\n",
    "model.fit(train_X,train_y)\n",
    "yhat = model.predict(watch_X)\n",
    "for i in range(len(yhat)):\n",
    "    if yhat[i] > 0.5:\n",
    "        yhat[i] = 1\n",
    "    else:\n",
    "        yhat[i] = 0\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(watch_y, yhat).ravel()\n",
    "min(tp/(tp+fn),tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the importance of each features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 25))\n",
    "plot_importance(model, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delete features which have 0 F score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select = []\n",
    "for item in list(model.feature_importances_):\n",
    "    select.append(bool(item))\n",
    "    \n",
    "train = train.iloc[:,select]\n",
    "test = test.drop(list(set(test).difference(set(train))),axis=1)\n",
    "watch = watch.drop(list(set(watch).difference(set(train))),axis=1)\n",
    "train_X = train.drop('In-hospital_death',axis=1)\n",
    "train_y = train['In-hospital_death']\n",
    "watch_X = watch.drop('In-hospital_death',axis=1)\n",
    "watch_y = watch['In-hospital_death']\n",
    "test_X = test.drop('In-hospital_death',axis=1)\n",
    "test_y = test['In-hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use recursive feature elimination to select the best number of features to use, in this case, no feature is deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfe = RFECV(xgb.XGBClassifier(max_depth=5,n_estimators=150,learning_rate=0.08,scale_pos_weight=3),scoring='roc_auc')\n",
    "rfe.fit(train,train['In-hospital_death'])\n",
    "train = train.iloc[:,rfe.get_support()]\n",
    "test = test.drop(list(set(test).difference(set(train))),axis=1)\n",
    "watch = watch.drop(list(set(watch).difference(set(train))),axis=1)\n",
    "train_X = train.drop('In-hospital_death',axis=1)\n",
    "train_y = train['In-hospital_death']\n",
    "watch_X = watch.drop('In-hospital_death',axis=1)\n",
    "watch_y = watch['In-hospital_death']\n",
    "test_X = test.drop('In-hospital_death',axis=1)\n",
    "test_y = test['In-hospital_death']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------ separate ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do bayesian optimization, tune the hyperparameters based on training dataset and watching dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = {'max_depth': hp.quniform('max_depth',2,5,1),\n",
    "         'learning_rate': hp.uniform('learning_rate',0.01,0.1),\n",
    "         'n_estimators': hp.quniform('n_estimators',80,150,1),\n",
    "         'gamma': hp.uniform('gamma',0,10),\n",
    "         'min_child_weight': hp.uniform('min_child_weight',0,5),\n",
    "         'max_delta_step': hp.uniform('max_delta_step',0,10),\n",
    "         'subsample': hp.uniform('subsample',0.5,1),\n",
    "         #'colsample_bytree': hp.uniform('colsample_bytree',0.5,1),\n",
    "         'reg_alpha': hp.uniform('reg_alpha',0,10),\n",
    "         'reg_lambda': hp.uniform('reg_lambda',0,10),\n",
    "         'scale_pos_weight': hp.uniform('scale_pos_weight',3,5)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    bst = xgb.XGBClassifier(max_depth=int(params['max_depth']),learning_rate=params['learning_rate'],\n",
    "                                n_estimators=int(params['n_estimators']),gamma=params['gamma'],\n",
    "                                min_child_weight=params['min_child_weight'],max_delta_step=params['max_delta_step'],\n",
    "                                subsample=params['subsample'], #colsample_bytree=params['colsample_bytree'], \n",
    "                                reg_alpha=params['reg_alpha'],reg_lambda=params['reg_lambda'],\n",
    "                                scale_pos_weight=params['scale_pos_weight'])\n",
    "    bst.fit(train_X,train_y)\n",
    "    yhat = bst.predict(watch_X)\n",
    "    for i in range(len(yhat)):\n",
    "        if yhat[i] > 0.5:\n",
    "            yhat[i] = 1\n",
    "        else:\n",
    "            yhat[i] = 0\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(watch_y, yhat).ravel()\n",
    "    return 1-min(tp/(tp+fn),tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once obtain the best hyperparameters evaluated on watching dataset, evaluate on the actual testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "param = fmin(objective,space,algo=tpe.suggest,max_evals=20)\n",
    "bst = xgb.XGBClassifier(max_depth=int(param['max_depth']),learning_rate=param['learning_rate'],\n",
    "                            n_estimators=int(param['n_estimators']),gamma=param['gamma'],\n",
    "                            min_child_weight=param['min_child_weight'],max_delta_step=param['max_delta_step'],\n",
    "                            subsample=param['subsample'], #colsample_bytree=param['colsample_bytree'], \n",
    "                            reg_alpha=param['reg_alpha'],reg_lambda=param['reg_lambda'],\n",
    "                            scale_pos_weight=param['scale_pos_weight'])\n",
    "bst.fit(train_X,train_y)\n",
    "yhat = bst.predict(test_X)\n",
    "for i in range(len(yhat)):\n",
    "    if yhat[i] > 0.5:\n",
    "        yhat[i] = 1\n",
    "    else:\n",
    "        yhat[i] = 0\n",
    "tn, fp, fn, tp = metrics.confusion_matrix(test_y, yhat).ravel()\n",
    "min(tp/(tp+fn),tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------- separate -----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the hyperparameters used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = open('param.txt','w')\n",
    "df.write(str(param))\n",
    "df.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(bst)\n",
    "shap_values = explainer.shap_values(test_X)\n",
    "shap.force_plot(explainer.expected_value, shap_values[3998,:], test_X.iloc[3998,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[19,:], test_X.iloc[19,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[0,:], test_X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = np.linspace(-4,4,100)\n",
    "plt.xlabel(\"Log odds of die\")\n",
    "plt.ylabel(\"Probability of die\")\n",
    "plt.title(\"How changes in log odds convert to probability of die\")\n",
    "plt.plot(xs, 1/(1+np.exp(-xs)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_inds = np.argsort(-np.sum(np.abs(shap_values), 0))\n",
    "for i in range(5):\n",
    "    shap.dependence_plot(top_inds[i], shap_values, test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('/Users/zhuangshui/Documents/3163/FIT3164-master/set-a/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
